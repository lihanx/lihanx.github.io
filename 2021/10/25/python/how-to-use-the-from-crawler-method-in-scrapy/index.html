<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="零、前言Crawler 类是 Scrapy 中的一个核心组件, 很多其他组件通过实现 from_crawler() 方法与 Crawler 类建立起联系.from_crawler() 方法为爬虫各组件提供了强大的功能扩展支持.本文将结合源码，对from_crawler()方法进行梳理总结. 全文主要围绕以下几个问题:  哪些组件可以使用 from_crawler() 方法? 在 from_cr">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy 中的 from_crawler() 方法应该怎么用?">
<meta property="og:url" content="https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/">
<meta property="og:site_name" content="技术与生活">
<meta property="og:description" content="零、前言Crawler 类是 Scrapy 中的一个核心组件, 很多其他组件通过实现 from_crawler() 方法与 Crawler 类建立起联系.from_crawler() 方法为爬虫各组件提供了强大的功能扩展支持.本文将结合源码，对from_crawler()方法进行梳理总结. 全文主要围绕以下几个问题:  哪些组件可以使用 from_crawler() 方法? 在 from_cr">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-10-25T16:00:00.000Z">
<meta property="article:modified_time" content="2022-07-26T06:40:41.442Z">
<meta property="article:author" content="李瀚翾">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Scrapy">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Scrapy 中的 from_crawler() 方法应该怎么用?</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 5.2.0"></head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/tags/">Tags</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2022/03/20/astro-photography/the-colorful-moon/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2020/09/14/setting-up-a-k8s-cluster-with-vm/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&text=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&is_video=false&description=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Scrapy 中的 from_crawler() 方法应该怎么用?&body=Check out this article: https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&name=Scrapy 中的 from_crawler() 方法应该怎么用?&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&t=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6%E3%80%81%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">零、前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%93%AA%E4%BA%9B%E7%BB%84%E4%BB%B6%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8-from-crawler-%E6%96%B9%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">一、哪些组件可以使用 from_crawler() 方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%9C%A8-from-crawler-%E6%96%B9%E6%B3%95%E4%B8%AD%E9%83%BD%E5%8F%AF%E4%BB%A5%E5%AE%9E%E7%8E%B0%E5%93%AA%E4%BA%9B%E5%8A%9F%E8%83%BD"><span class="toc-number">3.</span> <span class="toc-text">二、在 from_crawler() 方法中都可以实现哪些功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81from-crawler-%E6%96%B9%E6%B3%95%E6%98%AF%E6%80%8E%E6%A0%B7%E8%A2%AB%E8%B0%83%E7%94%A8%E7%9A%84"><span class="toc-number">4.</span> <span class="toc-text">三、from_crawler() 方法是怎样被调用的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">5.</span> <span class="toc-text">四、参考资料</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Scrapy 中的 from_crawler() 方法应该怎么用?
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">李瀚翾</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2021-10-25T16:00:00.000Z" itemprop="datePublished">2021-10-26</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Scrapy/">Scrapy</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Python/" rel="tag">Python</a>, <a class="tag-link-link" href="/tags/Scrapy/" rel="tag">Scrapy</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <a id="more"></a>
<!-- excerpt -->
<h2 id="零、前言"><a href="#零、前言" class="headerlink" title="零、前言"></a>零、前言</h2><p><code>Crawler</code> 类是 Scrapy 中的一个核心组件, 很多其他组件通过实现 <code>from_crawler()</code> 方法与 <code>Crawler</code> 类建立起联系.<br><code>from_crawler()</code> 方法为爬虫各组件提供了强大的功能扩展支持.<br>本文将结合源码，对<code>from_crawler()</code>方法进行梳理总结.</p>
<p>全文主要围绕以下几个问题:</p>
<ol>
<li>哪些组件可以使用 <code>from_crawler()</code> 方法?</li>
<li>在 <code>from_crawler()</code>  中都可以实现哪些功能?</li>
<li><code>from_crawler()</code> 方法是怎么被调用的?</li>
</ol>
<p><br/></p>
<h2 id="一、哪些组件可以使用-from-crawler-方法"><a href="#一、哪些组件可以使用-from-crawler-方法" class="headerlink" title="一、哪些组件可以使用 from_crawler() 方法"></a>一、哪些组件可以使用 <code>from_crawler()</code> 方法</h2><p>首先，常用的组件有：</p>
<ul>
<li><code>DupeFilter</code></li>
<li><code>Scheduler</code></li>
<li><code>Middlewares</code></li>
<li><code>Pipelines</code></li>
<li><code>Extensions</code></li>
<li><code>Spider</code></li>
</ul>
<p>根据场景，不常用的组件有：</p>
<ul>
<li><code>downloader.handlers</code></li>
<li><code>queues</code></li>
<li><code>dns resolver</code></li>
<li><code>log formatter</code></li>
</ul>
<p>常用组件其实可以参考 <code>scrapy_redis</code> 的实现，很容易就可以看到相关的用法.</p>
<p>或者可以参考 scrapy 本身模块的实现，项目中可参考的模块有：</p>
<ul>
<li><code>scrapy.dupefilter</code></li>
<li><code>scrapy.core.scheduler</code></li>
<li><code>scrapy.downloadermiddlewares</code></li>
<li><code>scrapy.spidermiddlewares</code></li>
<li><code>scrapy.core.downloader.handlers</code></li>
<li><code>scrapy.squeues</code></li>
<li><code>scrapy.resolver</code></li>
</ul>
<p><br/></p>
<h2 id="二、在-from-crawler-方法中都可以实现哪些功能"><a href="#二、在-from-crawler-方法中都可以实现哪些功能" class="headerlink" title="二、在 from_crawler() 方法中都可以实现哪些功能"></a>二、在 <code>from_crawler()</code> 方法中都可以实现哪些功能</h2><p>首先，<code>from_crawler()</code> 最核心的一个目的，是<strong>创建并返回其所在类的实例</strong>。</p>
<p>比如自己实现 Spider 时，所继承的父类 <code>scrapy.Spider</code>中:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider</span>(<span class="params">object_ref</span>):</span></span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="comment"># 创建所在类的实例并返回</span></span><br><span class="line">        spider = cls(*args, **kwargs)</span><br><span class="line">        spider._set_crawler(crawler)</span><br><span class="line">        <span class="keyword">return</span> spider</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里有个相关的知识点：类方法</p>
</blockquote>
<p>其次，由于必须传入<code>crawler</code> 实例作为参数，那么我们可以<strong>对 <code>crawler</code> 携带的属性加以利用</strong>。  </p>
<p>比如内置的爬虫中间件 <code>scrapy.spidermiddlewares.depth.DepthMiddleware</code>：</p>
<p><em>在 <code>from_crawler()</code> 中获取 settings 中的配置项，将其作为实例属性绑定到中间件实例中</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DepthMiddleware</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, maxdepth, stats, verbose_stats=<span class="literal">False</span>, prio=<span class="number">1</span></span>):</span></span><br><span class="line">        self.maxdepth = maxdepth</span><br><span class="line">        self.stats = stats</span><br><span class="line">        self.verbose_stats = verbose_stats</span><br><span class="line">        self.prio = prio</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="comment"># 可以获取 crawler.settings 中的配置项，在实例化时作为参数传入</span></span><br><span class="line">        settings = crawler.settings</span><br><span class="line">        maxdepth = settings.getint(<span class="string">&#x27;DEPTH_LIMIT&#x27;</span>)</span><br><span class="line">        verbose = settings.getbool(<span class="string">&#x27;DEPTH_STATS_VERBOSE&#x27;</span>)</span><br><span class="line">        prio = settings.getint(<span class="string">&#x27;DEPTH_PRIORITY&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(maxdepth, crawler.stats, verbose, prio)</span><br><span class="line">      </span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>另外，由于 scrapy 提供了信号的支持, 并且 <code>Crawler</code> 实例中绑定了一个 <code>signals</code> 属性。那么，我们可以 <strong>利用信号机制在特定时机执行特定的操作</strong>。</p>
<p>比如 <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/signals.html">Scrapy 文档 - Signals</a> 一节给出的爬虫示例:</p>
<p><em><code>from_crawler()</code> 定义了在爬虫结束时，执行 <code>spider_closed（）</code>方法，输出日志</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DmozSpider</span>(<span class="params">Spider</span>):</span></span><br><span class="line">    name = <span class="string">&quot;dmoz&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;dmoz.org&quot;</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span>,</span><br><span class="line">        <span class="string">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler, *args, **kwargs</span>):</span></span><br><span class="line">        spider = <span class="built_in">super</span>(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        <span class="comment"># 绑定 spider_closed 方法到信号 signals.spider_closed</span></span><br><span class="line">        <span class="comment"># 使其爬虫结束时触发执行</span></span><br><span class="line">        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)</span><br><span class="line">        <span class="keyword">return</span> spider</span><br><span class="line">      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        spider.logger.info(<span class="string">&#x27;Spider closed: %s&#x27;</span>, spider.name)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p><em>更多 <code>Crawler</code> 的 API 见文档 <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/api.html#crawler-api">Crawler API</a></em></p>
<h2 id="三、from-crawler-方法是怎样被调用的"><a href="#三、from-crawler-方法是怎样被调用的" class="headerlink" title="三、from_crawler() 方法是怎样被调用的"></a>三、<code>from_crawler()</code> 方法是怎样被调用的</h2><p>在上述示例中，直接在类里面声明了 <code>from_crawler()</code> 方法，它们会直接被 scrapy 内部流程中被调用。</p>
<p>那么具体调用过程是什么样的呢？</p>
<p>上面已经说过，类方法 <code>from_crawler()</code> 的主要功能是创建并返回类的实例，那么只要找到这些组件实例化的位置，就能看到 <code>from_crawler()</code> 被调用的过程。</p>
<p>这些组件被实例化，主要是通过两种方式：</p>
<ol>
<li>直接调用 <code>from_crawler()</code> 进行实例化</li>
<li>通过 <code>scrapy.util.misc.create_instance()</code> 函数进行实例化</li>
</ol>
<p>首先说 1. 直接调用 <code>from_crawler()</code> 进行实例化。</p>
<p>典型的示例如 <code>scrapy.crawler.Crawler</code> 中，对爬虫类 Spider 进行实例化的过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Crawler</span>:</span></span><br><span class="line">   ...</span><br><span class="line">  </span><br><span class="line"><span class="meta">   @defer.inlineCallbacks</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crawl</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.crawling:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Crawling already taking place&quot;</span>)</span><br><span class="line">        self.crawling = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.spider = self._create_spider(*args, **kwargs)</span><br><span class="line">            self.engine = self._create_engine()</span><br><span class="line">            start_requests = <span class="built_in">iter</span>(self.spider.start_requests())</span><br><span class="line">            <span class="keyword">yield</span> self.engine.open_spider(self.spider, start_requests)</span><br><span class="line">            <span class="keyword">yield</span> defer.maybeDeferred(self.engine.start)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            self.crawling = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> self.engine <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">yield</span> self.engine.close()</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_spider</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="comment"># 这里直接调用了 spidercls 的 from_crawler 方法，获取 spidercls 的实例</span></span><br><span class="line">        <span class="keyword">return</span> self.spidercls.from_crawler(self, *args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_engine</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ExecutionEngine(self, <span class="keyword">lambda</span> _: self.stop())</span><br><span class="line">      </span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>在 scrapy 中用到最多的是第二种，通过<code>create_instance()</code> 函数进行实例化.</p>
<p>调用的实例如 scheduler 实例化 dupefilter 的过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Scheduler</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        settings = crawler.settings</span><br><span class="line">        dupefilter_cls = load_object(settings[<span class="string">&#x27;DUPEFILTER_CLASS&#x27;</span>])</span><br><span class="line">        <span class="comment"># 这里使用 create_instance 对 dupefilter_cls 进行实例化</span></span><br><span class="line">        dupefilter = create_instance(dupefilter_cls, settings, crawler)</span><br><span class="line">        pqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_PRIORITY_QUEUE&#x27;</span>])</span><br><span class="line">        <span class="keyword">if</span> pqclass <span class="keyword">is</span> PriorityQueue:</span><br><span class="line">            warnings.warn(<span class="string">&quot;SCHEDULER_PRIORITY_QUEUE=&#x27;queuelib.PriorityQueue&#x27;&quot;</span></span><br><span class="line">                          <span class="string">&quot; is no longer supported because of API changes; &quot;</span></span><br><span class="line">                          <span class="string">&quot;please use &#x27;scrapy.pqueues.ScrapyPriorityQueue&#x27;&quot;</span>,</span><br><span class="line">                          ScrapyDeprecationWarning)</span><br><span class="line">            <span class="keyword">from</span> scrapy.pqueues <span class="keyword">import</span> ScrapyPriorityQueue</span><br><span class="line">            pqclass = ScrapyPriorityQueue</span><br><span class="line"></span><br><span class="line">        dqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_DISK_QUEUE&#x27;</span>])</span><br><span class="line">        mqclass = load_object(settings[<span class="string">&#x27;SCHEDULER_MEMORY_QUEUE&#x27;</span>])</span><br><span class="line">        logunser = settings.getbool(<span class="string">&#x27;SCHEDULER_DEBUG&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,</span><br><span class="line">                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass,</span><br><span class="line">                   mqclass=mqclass, crawler=crawler)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>那么我们来看下 <code>create_instance</code> 函数内部的实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_instance</span>(<span class="params">objcls, settings, crawler, *args, **kwargs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct a class instance using its ``from_crawler`` or</span></span><br><span class="line"><span class="string">    ``from_settings`` constructors, if available.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At least one of ``settings`` and ``crawler`` needs to be different from</span></span><br><span class="line"><span class="string">    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.</span></span><br><span class="line"><span class="string">    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be</span></span><br><span class="line"><span class="string">    tried.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ``*args`` and ``**kwargs`` are forwarded to the constructors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. versionchanged:: 2.2</span></span><br><span class="line"><span class="string">       Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an</span></span><br><span class="line"><span class="string">       extension has not been implemented correctly).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> settings <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> crawler <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Specify at least one of settings and crawler.&quot;</span>)</span><br><span class="line">        settings = crawler.settings</span><br><span class="line">    <span class="keyword">if</span> crawler <span class="keyword">and</span> <span class="built_in">hasattr</span>(objcls, <span class="string">&#x27;from_crawler&#x27;</span>):</span><br><span class="line">        instance = objcls.from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        method_name = <span class="string">&#x27;from_crawler&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">hasattr</span>(objcls, <span class="string">&#x27;from_settings&#x27;</span>):</span><br><span class="line">        instance = objcls.from_settings(settings, *args, **kwargs)</span><br><span class="line">        method_name = <span class="string">&#x27;from_settings&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        instance = objcls(*args, **kwargs)</span><br><span class="line">        method_name = <span class="string">&#x27;__new__&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> instance <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">f&quot;<span class="subst">&#123;objcls.__qualname__&#125;</span>.<span class="subst">&#123;method_name&#125;</span> returned None&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> instance</span><br></pre></td></tr></table></figure>
<p>逻辑非常清晰明了，根据条件判断调用类的 <code>from_crawler()</code> 方法 或 <code>from_settings()</code> 方法，对类进行实例化，并返回。</p>
<p>同时，这里也可以看到另一个常见的类方法—— <code>from_settings()</code> ，有着和 <code>from_crawler()</code> 相似的作用。</p>
<p>但是如果一个组件是通过 <code>create_instance()</code> 被实例化的, 并且组件内又同时实现了 <code>from_crawler()</code> 和 <code>from_settings()</code> 方法，那么将只有 <code>from_crawler()</code> 方法被调用。</p>
<p>至此, 我们可以对 Scrapy 中常用的 <code>from_crawler()</code> 方法有一个基本的认识.</p>
<p><br/></p>
<h2 id="四、参考资料"><a href="#四、参考资料" class="headerlink" title="四、参考资料"></a>四、参考资料</h2><p>[1] <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/index.html">Scrapy documentation</a><br>[2] Scrapy 源码</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/tags/">Tags</a></li>
         
          <li><a href="/categories/">Category</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6%E3%80%81%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">零、前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%93%AA%E4%BA%9B%E7%BB%84%E4%BB%B6%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8-from-crawler-%E6%96%B9%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">一、哪些组件可以使用 from_crawler() 方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%9C%A8-from-crawler-%E6%96%B9%E6%B3%95%E4%B8%AD%E9%83%BD%E5%8F%AF%E4%BB%A5%E5%AE%9E%E7%8E%B0%E5%93%AA%E4%BA%9B%E5%8A%9F%E8%83%BD"><span class="toc-number">3.</span> <span class="toc-text">二、在 from_crawler() 方法中都可以实现哪些功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81from-crawler-%E6%96%B9%E6%B3%95%E6%98%AF%E6%80%8E%E6%A0%B7%E8%A2%AB%E8%B0%83%E7%94%A8%E7%9A%84"><span class="toc-number">4.</span> <span class="toc-text">三、from_crawler() 方法是怎样被调用的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">5.</span> <span class="toc-text">四、参考资料</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&text=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&is_video=false&description=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Scrapy 中的 from_crawler() 方法应该怎么用?&body=Check out this article: https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&title=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&name=Scrapy 中的 from_crawler() 方法应该怎么用?&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://lihanx.github.io/2021/10/25/python/how-to-use-the-from-crawler-method-in-scrapy/&t=Scrapy 中的 from_crawler() 方法应该怎么用?"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2022
    李瀚翾
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/tags/">Tags</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
